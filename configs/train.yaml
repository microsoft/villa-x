defaults:
  - _self_
hydra:
  output_subdir: null
  run:
    dir: ${log_dir}
_target_: villax.trainer.trainer.Trainer

log_dir: ${oc.env:VLA_LOG_DIR}
name: ${oc.env:VLA_NAME}
device: cuda
n_nodes: 1
seed: 42

pretrained_model_path: ${oc.env:TRANSFORMERS_CACHE}
resume_checkpoint_path:
load_pretrained_weights: True
train_vlm: True
action_expert_adaptive_mode: # adaLN, adaLN-Zero, or None
use_torch_compile: True
use_bf16: True
use_amp: True
quantize: False
lora: False
lora_r: 32
lora_dropout: 0.0
debug: False
seq_tokenizer_use_2_frames: False

use_ema: False
ema_decay: 0.99
ema_start: ${save_model_start}
ema_freq: 1
ema_device: cuda
use_swa: False
swa_start: ${eval:'800000 // ${global_batch_size} * 5'}
swa_freq: ${eval:'800000 // ${global_batch_size} // 4'}
swa_device: cpu

n_datasets: None

id2stat:
data:
  config:
    name: "bridgert1"
    base_dir: ${oc.env:VLA_DATA_DIR}
    sample_length: 8
    resolution: [224, 224]
    action_chunk: ${real_action_n_tokens}
    use_normalization: true
    normalization_type: quantile
    skip_empty_instruction: true
    allow_pading: true
    padding_side: right
    min_valid_frames: 1
    min_right_samples: 1
    load_camera_views: ["primary", "wrist"]
    sample_interval: 3

wandb:
  entity: ${oc.env:VLA_WANDB_ENTITY}
  project: villax
  run: ${now:%H-%M-%S}_${name}

log_freq: 16
n_epochs: 500 # provided ckpts were about 12 epochs
n_updates: ${eval:'1550000 // ${global_batch_size} * ${n_epochs}'} # bridge dataset has 2195527 transitions in total, but we are skipping (text-)unlabeled episodes, which is roughly 30% of the data
save_model_freq: ${eval:'800000 // ${global_batch_size} * 1'}
save_model_start: ${eval:'800000 // ${global_batch_size} * 1'}
eval_freq: 1024
eval_size: 1024
eval_thresholds: [0.05, 0.1, 0.2, 0.3, 0.5]

la_attn_mask_ratio: 0.5

global_batch_size: 512
per_device_batch_size: 8
robot_action_lr: 5e-5
latent_action_lr: 5e-5 # (smaller due to stage 1)
vlm_lr: 5e-5 # (smaller due to stage 1)
robot_action_lr_scheduler:
  first_cycle_steps: 10000000 # basically no decaying
  min_lr: 1e-8
  warmup_steps: 200 # a bit of warmup
latent_action_lr_scheduler:
  first_cycle_steps: 10000000 # basically no decaying
  min_lr: 1e-8
  warmup_steps: 200 # longer warm up due to stage1
vlm_lr_scheduler:
  first_cycle_steps: 10000000
  min_lr: 1e-8
  warmup_steps: 200 # longer warm up due to stage1
robot_action_weight_decay: 0
latent_action_weight_decay: 0
vlm_weight_decay: 0
max_grad_norm: 1.0

flow_sampling: sepbeta
num_inference_steps: 10
final_action_clip_value: 1.0 # data normalized in [-1,1]

flow_alpha1: 1.5
flow_beta1: 1.0
flow_alpha2: 5.0
flow_beta2: 1.0

dataset_cond_steps: 8 # #images loading from dataloader
dataset_cond_hist_steps: 0
# dataset_horizon_steps: 12  # #actions loading from dataloader
proprio_cond_steps: 1
latent_action_n_tokens: 12
latent_action_n_tokens_hist: 0
real_action_n_tokens: 4
latent_action_dim: 32 #
robot_action_dim: 7
proprio_dim: 7 # POS_EULER
max_seq_len: 276 # fixed 256 for image + max 20 for text
tokenizer_padding: max_length # instead of truncating to longest
max_image_text_tokens: ${max_seq_len}

latent_action_loss_coef: 1.0

seq_tokenizer:
  _target_: villax.tokenizer.lam.IgorLAM
  config:
    use_continuous: True
    dataset_statistics_path: config/train_statistics.json
    ckpt_dir: ./checkpoints/lam

stage1_weights: None

# Image Encoder Configuration
robot_image_encoder_tokens: 16
robot_image_encoder:
  pretrained:
    _target_: villax.model.hpt.ResNet
    resnet_model: "resnet18"
  heads: 8
  dim_head: 256
  query_dim: 512

mixture:
  vlm: # gemma
    hidden_size: 2048
    intermediate_size: 16384
    use_final_norm: False
    cache: True
    use_quantize: ${quantize}
    use_lora: ${lora}
    adaptive_mode: # not applicable for gemma
    rope_theta: 10000.0 # 10000 in gemma
  latent_action:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True # technically no, but sharing weights with action anyway
    cache: False
    use_quantize: False
    use_lora: False
    adaptive_mode: ${action_expert_adaptive_mode}
    rope_theta: ${action_expert_rope_theta}
  proprio:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True # technically no, but sharing weights with action anyway
    cache: False
    use_quantize: False
    use_lora: False
    adaptive_mode: ${action_expert_adaptive_mode}
    rope_theta: ${action_expert_rope_theta}
  action:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True
    cache: False
    use_quantize: False
    use_lora: False
    adaptive_mode: ${action_expert_adaptive_mode}
    rope_theta: ${action_expert_rope_theta}
time_hidden_size: 256 # only applicable if using adaptive
time_max_period: 100.0
action_expert_rope_theta: 100.0 # since action/proprio seq_len is pretty small

# Fixed
image_token_index: 257152
vocab_size: 257216
pad_token_id: 0

vision:
  _target_: villax.model.paligemma.siglip.SiglipVisionModel
  config:
    hidden_size: 1152 # siglip
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    layer_norm_eps: 1e-6
    attention_dropout: 0.0
    num_image_tokens: 256
    lora:
      r: ${lora_r}
      dropout: ${lora_dropout}
  use_quantize: ${quantize}
  use_lora: ${lora}

vision_projector:
  _target_: villax.model.paligemma.siglip.PaliGemmaMultiModalProjector
  config:
    vision_config:
      hidden_size: 1152
      projection_dim: 2048
    lora:
      r: ${lora_r}
      dropout: ${lora_dropout}
  use_quantize: ${quantize}
  use_lora: ${lora}

joint:
  _target_: villax.model.vla.joint_model.JointModel
  config:
    action_expert_adaptive_mode: ${action_expert_adaptive_mode}
    time_hidden_size: ${time_hidden_size}
    mixture: ${mixture}
    lora:
      r: ${lora_r}
      dropout: ${lora_dropout}
    #
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    rms_norm_eps: 1e-6
    attention_bias: False
    attention_dropout: 0.0
    pad_token_id: ${pad_token_id}
